{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team members:\n",
    "\n",
    "- Technical writer - Arman\n",
    "- Designer of interactive plots - Sabina\n",
    "- Designer of quizzes - Aidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is activation functions?\n",
    "\n",
    "Activation functions decide whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. They are differentiable operators for transforming input signals to outputs, while most of them add nonlinearity. Because activation functions are fundamental to deep learning, letâ€™s briefly survey some common ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} images/activation_f.gif\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do Neural Networks Need an Activation Function?\n",
    "\n",
    "Activation functions introduce an additional step at each layer during the forward propagation, but its computation is worth it. \n",
    "\n",
    "Here is why:\n",
    "Letâ€™s suppose we have a neural network working without the activation functions. In that case, every neuron will only be performing a linear transformation on the inputs using the weights and biases. Itâ€™s because it doesnâ€™t matter how many hidden layers we attach in the neural network; all layers will behave in the same way because the composition of two linear functions is a linear function itself. Although the neural network becomes simpler, learning any complex task is impossible, and our model would be just a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ Activation Function helps the neural network to use important information while suppressing irrelevant data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Types of Neural Networks Activation Functions\n",
    "\n",
    "Let's examine the most popular types of neural network activation functions to solidify our knowledge of activation functions in practice. The three most popular functions are:\n",
    "\n",
    "1. Binary step\n",
    "2. Linear activation\n",
    "3. Non-linear activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary step activation function\n",
    "\n",
    "Binary step function depends on a threshold value that decides whether a neuron should be activated or not. The input fed to the activation function is compared to a certain threshold; if the input is greater than it, then the neuron is activated, else it is deactivated, meaning that its output is not passed on to the next hidden layer.\n",
    "\n",
    "```{math}\n",
    ":label: binary_step\n",
    "    f(x) =\n",
    "    \\begin{cases}\n",
    "    0, & \\text{if } x < 0,\\\\\n",
    "    1, & \\text{if } x \\geq 0.\n",
    "    \\end{cases}\n",
    "```\n",
    "\n",
    "```{admonition} Question\n",
    ":class: important\n",
    "What are some limitations of using a binary step function?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Sabina plot with slider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear activation function\n",
    "\n",
    "The linear activation function, also referred to as \"no activation\" or \"identity function,\" is a function where the activation is directly proportional to the input. This function does not modify the weighted sum of the input and simply returns the value it was given. \n",
    "\n",
    "```{math}\n",
    ":label: linear_activation\n",
    "    f(x) = x\n",
    "```\n",
    "\n",
    "\n",
    "```{admonition} Question\n",
    ":class: important\n",
    "What are some limitations of using a linear activation function?\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Sabina plot with slider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Linear activation function\n",
    "\n",
    "The linear activation function is equivalent to a linear regression model. This is because the linear activation function simply outputs the input that it receives, without applying any transformation.\n",
    "\n",
    "In a neural network, the output of a neuron is computed using the following equation:\n",
    "\n",
    "```{math}\n",
    ":label: non_linear_act\n",
    "    output = activation(inputs * weights + bias)\n",
    "```\n",
    "\n",
    "Non-linear activation functions overcome linear ones by enabling backpropagation through input-dependent derivatives and supporting deep, complex architectures with non-linear input combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Sabina plot with slider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "\n",
    "This function takes any real value as input and outputs values in the range of 0 to 1. \n",
    "\n",
    "The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0.\n",
    "\n",
    "\n",
    "```{math}\n",
    ":label: sigmoid\n",
    "    \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "```\n",
    "\n",
    "Derivative of sigmoid function:\n",
    "\n",
    "```{math}\n",
    ":label: sigmoid_derivative\n",
    "    \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "```\n",
    "\n",
    "As we can see from the above Figure, the gradient values are only significant for range -3 to 3, and the graph gets much flatter in other regions. \n",
    "\n",
    "It implies that for values greater than 3 or less than -3, the function will have very small gradients. As the gradient value approaches zero, the network ceases to learn and suffers from the [Vanishing gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) problem.\n",
    "\n",
    "```{admonition} Question\n",
    ":class: important\n",
    "Is the output of the sigmoid activation function symmetrical around zero?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Sabina plot with slider for sigmoid and regular plot for derivative of sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh function (Hyperbolic Tangent)\n",
    "\n",
    "The tanh activation function is similar to the sigmoid in that it maps input values to an s-shaped curve. But, in the tanh function, the range is (-1, 1) and is centered at 0. This addresses one of the issues with the sigmoid function.\n",
    "\n",
    "```{math}\n",
    ":label: tanh\n",
    "    \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "```\n",
    "\n",
    "The output of the tanh activation function is [Zero centered](https://stackoverflow.com/questions/59540276/why-in-preprocessing-image-data-we-need-to-do-zero-centered-data); hence we can easily map the output values as strongly negative, neutral, or strongly positive. \n",
    "\n",
    "```{admonition} Question\n",
    ":class: important\n",
    "Why is it good for neural networks when the activation function is a zero centered?\n",
    "```\n",
    "\n",
    "Derivative of tanh function:\n",
    "\n",
    "```{math}\n",
    ":label: tanh_derivative\n",
    "\\tanh'(x) = 1 - \\tanh^2(x)\n",
    "```\n",
    "\n",
    "As you can see - it also faces the problem of vanishing gradients similar to the sigmoid activation function. Plus the gradient of the tanh function is much steeper as compared to the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Although both sigmoid and tanh face vanishing gradient issue, tanh is zero centered, and the gradients are not restricted to move in a certain direction. Therefore, in practice, tanh nonlinearity is always preferred to sigmoid nonlinearity.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Sabina plot with slider for tanh and regular plot for derivative of tanh function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU function\n",
    "\n",
    "ReLU (Rectified Linear Unit) is a more modern and widely used activation function. ReLU is a simple activation function that replaces negative values with 0 and leaves positive values unchanged, which helps avoid issues with gradients during backpropagation and is faster computationally.\n",
    "\n",
    "```{math}\n",
    ":label: relu\n",
    "f(x) = \\max(0, x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Sabina plot with slider for relu and regular plot for derivative of relu function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dying ReLU problem\n",
    "\n",
    "\n",
    "Derivative of ReLU function:\n",
    "\n",
    "```{math}\n",
    ":label: relu_derivative\n",
    "f'(x) = \n",
    "\\begin{cases} \n",
    "0 & \\text{if } x < 0 \\\\\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "\\text{undefined} & \\text{if } x = 0 \n",
    "\\end{cases}\n",
    "```\n",
    "\n",
    "The negative side of the ReLU it makes the gradient value zero. Due to this reason, during the backpropagation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "Leaky ReLU is an improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area.\n",
    "\n",
    "```{math}\n",
    ":label: leaky_relu\n",
    "f(x) = \\max(0.01x, x)\n",
    "```\n",
    "\n",
    "The advantages of Leaky ReLU are same as that of ReLU, in addition to the fact that it does enable backpropagation, even for negative input values. \n",
    "\n",
    "```{admonition} Limitations\n",
    ":class: warning\n",
    "The predictions may not be consistent for negative input values.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric ReLU\n",
    "\n",
    "Parametric ReLU provides the slope of the negative part of the function as an argument $\\alpha$. By performing backpropagation, the most appropriate value of $\\alpha$\n",
    "is learnt.\n",
    "\n",
    "```{math}\n",
    ":label: parametric_relu\n",
    "f(x) = \\max(\\alpha x, x)\n",
    "```\n",
    "\n",
    "The parameterized ReLU function is used when the Leaky ReLU function still fails at solving the problem of dead neurons, and the relevant information is not successfully passed to the next layer. \n",
    "\n",
    "```{admonition} Limitations\n",
    ":class: warning\n",
    "Limitations: Function may perform differently for different problems depending upon the value of slope parameter a.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax\n",
    "\n",
    "Before exploring the ins and outs of the Softmax activation function, we should focus on its building block: the sigmoid/logistic activation function that works on calculating probability values. \n",
    "\n",
    "The output of the sigmoid function was in the range of 0 to 1, which can be thought of as probability. \n",
    "\n",
    "```{admonition} Question\n",
    ":class: important\n",
    "Letâ€™s suppose we have five output values of 0.8, 0.9, 0.7, 0.8, and 0.6, respectively. How can we move forward with it?\n",
    "```\n",
    "\n",
    "The Softmax function is described as a combination of multiple sigmoids. It calculates the relative probabilities. Similar to the sigmoid/logistic activation function, the SoftMax function returns the probability of each class. \n",
    "\n",
    "```{math}\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "```\n",
    "\n",
    "```{note}\n",
    "SoftMax is most commonly used as an activation function for the last layer of the neural network in the case of multi-class classification. \n",
    "```\n",
    "\n",
    "````{admonition} Simple example how to work SoftMax:\n",
    ":class: dropdown\n",
    "\n",
    "Assume that you have three classes, meaning that there would be three neurons in the output layer. Now, suppose that your output from the neurons is [1.8, 0.9, 0.68].\n",
    "\n",
    "Applying the softmax function over these values to give a probabilistic view will result in the following outcome: [0.58, 0.23, 0.19]. \n",
    "\n",
    "The function returns 1 for the largest probability index while it returns 0 for the other two array indexes. So the output would be the class corresponding to the 1st neuron(index 0) out of three.\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
